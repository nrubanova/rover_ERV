{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import seed\n",
    "from random import sample\n",
    "import pyfastx\n",
    "import pysam\n",
    "import os.path\n",
    "import re\n",
    "import csv\n",
    "from Bio.Align import MultipleSeqAlignment\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqIO\n",
    "import mappy as mp\n",
    "import uuid as uuid_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "chrms = ['chr2L', 'chr2R', 'chr3L', 'chr3R', 'chrX', 'chr4', 'chrM']\n",
    "dm6 = {}\n",
    "fastafile = 'dm6.fa'\n",
    "total_length = 0\n",
    "for seq_record in SeqIO.parse(fastafile, 'fasta'):\n",
    "    chrm = seq_record.id\n",
    "    dm6[chrm] = seq_record.seq\n",
    "    total_length = total_length + len(seq_record.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt_complement = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C', 'N': 'N'}\n",
    "def get_reverse_complement(seq):\n",
    "    revcompl = ''\n",
    "    for nt in reversed(seq):\n",
    "        revcompl = revcompl + nt_complement[nt]\n",
    "    return revcompl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt_alphabet = ['A', 'C', 'G', 'T']\n",
    "twomers = [x+y for x in nt_alphabet for y in nt_alphabet]\n",
    "def check_for_low_complexity(string):\n",
    "    length = 0.7 * len(string)\n",
    "    flag = 0\n",
    "    for tmer in twomers:\n",
    "        if len(string.replace(tmer, '')) < length:\n",
    "            flag = 1\n",
    "            break\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "germline_active = ['hobo', 'BS', 'roo', 'Tabor', 'opus', 'mdg1', 'FB', 'jockey', \n",
    "            'F-element', '1360', 'HeT-A', 'Doc', 'Tirant', 'flea', 'Stalker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesreformat = []\n",
    "for order in tes.keys():\n",
    "    for superfamily in tes[order]:\n",
    "        for subfamily in tes[order][superfamily]:\n",
    "            tesreformat.append([order, superfamily, subfamily, len(str(tes[order][superfamily][subfamily]))])\n",
    "tedf = pd.DataFrame(tesreformat, columns=['order', 'superfamily', 'subfamily', 'len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teTELR = '/D_mel_transposon_sequence_set.fa' # from https://github.com/bergmanlab/drosophila-transposons/blob/master/current/D_mel_transposon_sequence_set.fa\n",
    "\n",
    "def load_ref_te(teTELR):\n",
    "    tes = {}\n",
    "    tes_by_subfamily = {}\n",
    "    tes_superfamily_order = {}\n",
    "    \n",
    "    for seq_record in SeqIO.parse(teTELR, 'fasta'):\n",
    "        seqid = seq_record.id\n",
    "        ln = seqid.split('#')\n",
    "        subfamily = ln[0]\n",
    "        order = ln[1].split('/')[0]\n",
    "        superfamily = ln[1].split('/')[1]\n",
    "        if order not in tes:\n",
    "            tes[order] = {}\n",
    "        if superfamily not in tes[order]:\n",
    "            tes[order][superfamily] = {}\n",
    "        \n",
    "        if superfamily not in tes_superfamily_order:\n",
    "            tes_superfamily_order[superfamily] = order\n",
    "        tes[order][superfamily][subfamily] = seq_record.seq\n",
    "        tes_by_subfamily[subfamily] = seq_record.seq\n",
    "\n",
    "    ref_te_len = {}\n",
    "    for order in tes.keys():\n",
    "        for superfamily in tes[order]:\n",
    "            for subfamily in tes[order][superfamily]:            \n",
    "                ref_te_len[subfamily] = len(str(tes[order][superfamily][subfamily]))\n",
    "    \n",
    "    return tes, tes_by_subfamily, ref_te_len, tes_superfamily_order\n",
    "\n",
    "tes, tes_by_subfamily, ref_te_len, tes_superfamily_order = load_ref_te(teTELR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tldr_file(tldr_infile, outfile, sample, ref_te_len):\n",
    "    with open(tldr_infile, 'r') as indata, open(outfile, 'w') as outdata:\n",
    "        for i, l in enumerate(indata):\n",
    "\n",
    "            if i == 0:\n",
    "                outdata.write(l.rstrip() + '\\t'.join(['\\tSample', 'Left_flank', 'TE', \n",
    "                                                      'Right_flank', 'Ref_length']) + '\\n')\n",
    "                continue\n",
    "            ln = l.rstrip().split('\\t')\n",
    "            subfamily = ln[6]\n",
    "            if subfamily =='NA':\n",
    "                continue\n",
    "            \n",
    "            consensus = ln[21]\n",
    "            right_flank = consensus.split('\u001b[0m\u001b[91m')[1].replace('\u001b[0m','')\n",
    "            left_flank = consensus.split('\u001b[0m\u001b[91m')[0].split('\u001b[0m\u001b[33m')[0].replace('\u001b[91m','')\n",
    "            te = consensus.split('\u001b[0m\u001b[91m')[0].split('\u001b[0m\u001b[33m')[1].replace('\u001b[34m','').replace('\u001b[33m','')\n",
    "            outdata.write(l.rstrip() + '\\t'.join(['\\t' + sample, left_flank, te, right_flank, str(ref_te_len[subfamily])]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['PGFP_5d_guts_P2', 'PGFP_5d_heads_P2', \n",
    "           'PGFP_5d_guts_P3', 'PGFP_5d_heads_P3',\n",
    "           'PGFP_25d_guts_P1', 'PGFP_25d_guts_P2', \n",
    "           'PGFP_25d_heads_P1', 'PGFP_25d_heads_P2', \n",
    "           'PGFP_50d_guts_P1', 'PGFP_50d_heads_P1',\n",
    "           'PGFP_50d_guts_P2', 'PGFP_50d_heads_P2']\n",
    "\n",
    "sample_order = ['PGFP_5d_guts_P2', 'PGFP_5d_heads_P2', \\\n",
    "                'PGFP_5d_guts_P3', 'PGFP_5d_heads_P3',\n",
    "                'PGFP_25d_guts_P1', 'PGFP_25d_heads_P1', \\\n",
    "                'PGFP_25d_guts_P2', 'PGFP_25d_heads_P2', \\\n",
    "                'PGFP_50d_guts_P1', 'PGFP_50d_heads_P1', \\\n",
    "               'PGFP_50d_guts_P2', 'PGFP_50d_heads_P2']\n",
    "\n",
    "sample_map = {'PGFP_5d_guts_P2': 'PGFP_5d_heads_P2', 'PGFP_5d_heads_P2': 'PGFP_5d_guts_P2',\n",
    "            'PGFP_5d_guts_P3':  'PGFP_5d_heads_P3',  'PGFP_5d_heads_P3': 'PGFP_5d_guts_P3',\n",
    "            'PGFP_25d_guts_P1': 'PGFP_25d_heads_P1', 'PGFP_25d_heads_P1': 'PGFP_25d_guts_P1',\n",
    "            'PGFP_25d_guts_P2': 'PGFP_25d_heads_P2', 'PGFP_25d_heads_P2': 'PGFP_25d_guts_P2',\n",
    "            'PGFP_50d_guts_P1': 'PGFP_50d_heads_P1', 'PGFP_50d_heads_P1': 'PGFP_50d_guts_P1',\n",
    "            'PGFP_50d_guts_P2': 'PGFP_50d_heads_P2', 'PGFP_50d_heads_P2': 'PGFP_50d_guts_P2'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in samples:\n",
    "    tldr_infile = '~/' + sample + '.dm6.porechop.sorted.flt.table.txt'\n",
    "    tldr_outfile = '~/' + sample + '.dm6.porechop.sorted.flt.table.reformat.txt'\n",
    "    preprocess_tldr_file(tldr_infile, tldr_outfile, sample, ref_te_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tldr = {}\n",
    "for sample in samples:\n",
    "    tldrfile = '~/' + sample + '.dm6.porechop.sorted.flt.table.reformat.txt'\n",
    "    tldr[sample] = pd.read_csv(tldrfile, sep='\\t', header=0)\n",
    "        \n",
    "tldr_not_flt = pd.concat(tldr.values())\n",
    "tldr_not_flt.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tldr = tldr_not_flt[(tldr_not_flt['Subfamily'].notnull()) & \n",
    "                    (tldr_not_flt['UnmapCover'] > 0.5 )  &\n",
    "                    (tldr_not_flt['LengthIns'] > 500) &\n",
    "                   (tldr_not_flt['MedianMapQ'] >= 30) &\n",
    "                   (tldr_not_flt['Chrom'] != 'chrY')]\n",
    "tldr.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tldr[['tmp','EmptyReads']] = tldr['EmptyReads'].str.split('|',expand=True)\n",
    "tldr['EmptyReads'] = tldr['EmptyReads'].fillna(0)\n",
    "tldr['EmptyReads'] = tldr['EmptyReads'].astype(int)\n",
    "tldr['UsedReads'] = tldr['UsedReads'].astype(int)\n",
    "tldr['Coverage'] = tldr['UsedReads'] + tldr['EmptyReads']\n",
    "tldr['AF'] = tldr['UsedReads'] / tldr['Coverage']\n",
    "tldr = tldr.drop('tmp', axis=1)\n",
    "tldr = tldr[tldr['Coverage'] >= 10]\n",
    "tldr.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supp_data = {}\n",
    "row_data = {}\n",
    "hash_data = {}\n",
    "coords_data = {}\n",
    "\n",
    "for index, row in tldr.iterrows():\n",
    "    uuid = row['UUID']\n",
    "    chrm = row['Chrom']\n",
    "    start = row['Start']\n",
    "    end = row['End']\n",
    "    strand = row['Strand']\n",
    "    subfamily = row['Subfamily']\n",
    "    \n",
    "    unmapcov = row['UnmapCover']\n",
    "    ref_length = row['Ref_length']\n",
    "    length_ins = row['LengthIns'] \n",
    "    sample = row['SampleReads'].split('.')[0]\n",
    "    \n",
    "    found = 0\n",
    "    imprecise = 0\n",
    "    \n",
    "    conservation_val = round(length_ins * unmapcov / ref_length, 3)\n",
    "        \n",
    "    if length_ins * unmapcov / ref_length >= 0.8:\n",
    "            conservation = 'FL'\n",
    "    elif (length_ins * unmapcov / ref_length >= 0.7) and (length_ins * unmapcov / ref_length < 0.8):\n",
    "            conservation = 'AFL'\n",
    "    else:\n",
    "            conservation = 'TR'\n",
    "                                    \n",
    "    if (chrm, strand, start, end, subfamily) in hash_data:\n",
    "        key = hash_data[(chrm, strand, start, end, subfamily)] \n",
    "        row_data[key].append(row)\n",
    "        supp_data[key].append((sample, conservation))\n",
    "        coords_data[key].append((chrm, strand, start, end, subfamily,\n",
    "                                 conservation, conservation_val, uuid, key))\n",
    "    else:\n",
    "        for i in range(start - 100, start + 100):\n",
    "            for j in range(end - 100, end + 100):\n",
    "                if (chrm, strand, i, j, subfamily) in hash_data:\n",
    "                    key = hash_data[(chrm, strand, i, j, subfamily)]\n",
    "                    hash_data[(chrm, strand, start, end, subfamily)] = key\n",
    "                    row_data[key].append(row)\n",
    "                    supp_data[key].append((sample, conservation))\n",
    "                    coords_data[key].append((chrm, strand, start, end, subfamily,\n",
    "                                             conservation, conservation_val, uuid, key))\n",
    "                    found = 1\n",
    "                    break\n",
    "            if found == 1:\n",
    "                break\n",
    "        if found == 0:\n",
    "            key = uuid_gen.uuid4().hex\n",
    "            hash_data[(chrm, strand, start, end, subfamily)] = key\n",
    "            row_data[key] = []\n",
    "            supp_data[key] = []\n",
    "            coords_data[key] = []\n",
    "            row_data[key].append(row)\n",
    "            supp_data[key].append((sample, conservation))\n",
    "            coords_data[key].append((chrm, strand, start, end, subfamily, \n",
    "                                     conservation, conservation_val, uuid, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_by_chrom = {}\n",
    "skey = uuid_gen.uuid4().hex\n",
    "\n",
    "for key in coords_data:\n",
    "    coords_data[key].sort(key=lambda x: x[2])\n",
    "\n",
    "for key in coords_data:\n",
    "    chrm = coords_data[key][0][0]\n",
    "    strand = coords_data[key][0][1]\n",
    "    subfamily = coords_data[key][0][4]\n",
    "    if (chrm, strand, subfamily) not in clusters_by_chrom:\n",
    "        clusters_by_chrom[(chrm, strand, subfamily)] = []\n",
    "    \n",
    "    clusters_by_chrom[(chrm, strand, subfamily)].append(coords_data[key])\n",
    "    \n",
    "for (chrm, strand, subfamily) in clusters_by_chrom:\n",
    "    clusters_by_chrom[(chrm, strand, subfamily)].sort(key=lambda x: x[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sclusters_merged = {}\n",
    "wiggly_breakpoints = []\n",
    "normal_breakpoints = []\n",
    "\n",
    "for (chrm, strand, subfamily) in clusters_by_chrom:\n",
    "    skey = uuid_gen.uuid4().hex\n",
    "    sclusters_merged[skey] = []\n",
    "    prev_cluster_start = [0]\n",
    "    prev_key = skey\n",
    "    for cluster in clusters_by_chrom[(chrm, strand, subfamily)]:\n",
    "        if (abs(cluster[0][2] - np.mean(prev_cluster_start)) <= 300) :\n",
    "            sclusters_merged[skey].append(cluster)\n",
    "            prev_cluster_start.append(cluster[0][2])\n",
    "            if (abs(cluster[0][2] - np.mean(prev_cluster_start)) > 200 ):\n",
    "                if (skey not in wiggy_breakpoints):\n",
    "                    wiggly_breakpoints.append(skey)\n",
    "            else:\n",
    "                if (skey not in normal_breakpoints):\n",
    "                    normal_breakpoints.append(skey)\n",
    "        else:\n",
    "            skey = uuid_gen.uuid4().hex\n",
    "            sclusters_merged[skey] = []\n",
    "            prev_cluster_start = [cluster[0][2]]\n",
    "            prev_key = skey\n",
    "            sclusters_merged[skey].append(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tldr_merged_list = []\n",
    "skip_coverage = 0\n",
    "skip_coverage_deep = 0\n",
    "skip_rr = 0\n",
    "skip_remappable = 0 \n",
    "index = 0\n",
    "count_rr_outlier = 0\n",
    "keys_to_check = []\n",
    "\n",
    "for skey in sclusters_merged:\n",
    "    keys = []\n",
    "    preserved = []\n",
    "    chrm = ''\n",
    "    starts = []\n",
    "    ends = []\n",
    "    strand = ''\n",
    "    family = ''\n",
    "    subfamily = ''\n",
    "    starttes = []\n",
    "    endtes = []\n",
    "    reflength = 0\n",
    "    samples = []\n",
    "    telengths = {}\n",
    "    mapqs = {}\n",
    "    uuids = {}\n",
    "    usedreads = {}\n",
    "    spanreads = {}\n",
    "    coverage = {}\n",
    "    TSDs = {}\n",
    "    remappable_flag = {}\n",
    "    filter_flag = {}\n",
    "    \n",
    "    for cluster in sclusters_merged[skey]:\n",
    "        for insertion in cluster:\n",
    "            preserved.append(insertion[5])\n",
    "            key = insertion[8]\n",
    "            if key not in keys:\n",
    "                keys.append(key)\n",
    "        \n",
    "    if ('FL' not in preserved) and ('AFL' not in preserved):\n",
    "        continue\n",
    "        \n",
    "    for sample in sample_order:\n",
    "        telengths[sample] = []\n",
    "        mapqs[sample] = []\n",
    "        uuids[sample] = []\n",
    "        usedreads[sample] = []\n",
    "        spanreads[sample] = []\n",
    "        coverage[sample] = []\n",
    "        TSDs[sample] = []\n",
    "        remappable_flag[sample] = []\n",
    "        filter_flag[sample] = []\n",
    "\n",
    "    for key in keys:\n",
    "        for row in row_data[key]:\n",
    "            tsd = ''\n",
    "            \n",
    "            sample = row['SampleReads'].split('.')[0]\n",
    "            if sample not in samples:\n",
    "                samples.append(sample)\n",
    "            chrm = row['Chrom']\n",
    "\n",
    "            strand =row['Strand']\n",
    "            family = row['Family']\n",
    "            order = tes_superfamily_order[family]\n",
    "            subfamily = row['Subfamily']\n",
    "            reflength = row['Ref_length']\n",
    "            umapcov = row['UnmapCover']\n",
    "            starts.append(row['Start'])\n",
    "            ends.append(row['End'])\n",
    "            \n",
    "            starttes.append(row['StartTE'])\n",
    "            endtes.append(row['EndTE'])\n",
    "            \n",
    "            uuids[sample].append(row['UUID'])\n",
    "            telengths[sample].append(str(round(row['LengthIns'] * umapcov)))            \n",
    "            mapqs[sample].append(str(row['MedianMapQ']))\n",
    "            usedreads[sample].append(row['UsedReads'])\n",
    "            spanreads[sample].append(row['SpanReads'])\n",
    "            coverage[sample].append(row['Coverage'])\n",
    "            \n",
    "            if (not isinstance(row['TSD'], str)) and np.isnan(row['TSD']):\n",
    "                tsd = 'noTSD'\n",
    "            elif len(row['TSD']) > 50:\n",
    "                tsd = 'longTSD'\n",
    "            else:\n",
    "                tsd = row['TSD']\n",
    "            TSDs[sample].append(tsd)\n",
    "            remappable_flag[sample].append(str(row['Remappable']))\n",
    "            filter_flag[sample].append(row['Filter'])\n",
    "            \n",
    "    tissue = ''\n",
    "    \n",
    "    tmpvar = ''.join(samples)\n",
    "    if ('guts' in tmpvar) and ('heads' in tmpvar):\n",
    "        tissue = 'sboth'\n",
    "    elif ('guts' in tmpvar):\n",
    "        tissue = 'gut'\n",
    "    elif ('heads' in tmpvar):\n",
    "        tissue = 'head'\n",
    "    else:\n",
    "        tissue = 'unknown'\n",
    "        \n",
    "    new_row = []\n",
    "    \n",
    "    tmp_uuid = []\n",
    "    tmp_sample = []\n",
    "    tmp_usedreads = []\n",
    "    tmp_spanreads = []\n",
    "    tmp_coverage = []\n",
    "    tmp_TSD = []\n",
    "    tmp_remappable = []\n",
    "    tmp_filter = []\n",
    "    tmp_telength = []\n",
    "    tmp_mapq = []\n",
    "    \n",
    "    coverage_filter = 0\n",
    "    coverage_threshold = 15\n",
    "    coverage_max_threshold = 200\n",
    "    remappable_filter = 0\n",
    "    coverage_deep_filter = 0\n",
    "    \n",
    "    for sample in sample_order:\n",
    "        if sample in samples:\n",
    "            tmp_uuid.append('|'.join(uuids[sample]))\n",
    "        else:\n",
    "            tmp_uuid.append('-')\n",
    "            \n",
    "    for sample in sample_order:\n",
    "        if sample in samples:\n",
    "            tmp_sample.append(sample)\n",
    "        else:\n",
    "            tmp_sample.append('-')\n",
    "                        \n",
    "    for sample in sample_order:\n",
    "        if sample in samples:\n",
    "            tmp_usedreads.append(str(sum(usedreads[sample])))  \n",
    "            if len(usedreads[sample]) > 1:\n",
    "                if skey not in keys_to_check:\n",
    "                    keys_to_check.append(skey)\n",
    "        else:\n",
    "            tmp_usedreads.append('0')\n",
    "                        \n",
    "    for sample in sample_order:\n",
    "        if sample in samples:\n",
    "            tmp_spanreads.append(str(sum(spanreads[sample]))) \n",
    "        else:\n",
    "            tmp_spanreads.append('0')\n",
    "                        \n",
    "    for sample in sample_order:\n",
    "        if sample in samples:\n",
    "            tmp_coverage.append(str(max(coverage[sample])))\n",
    "            if max(coverage[sample]) >= coverage_threshold:\n",
    "                coverage_filter = 1 \n",
    "        else:\n",
    "            tmp_coverage.append('0')\n",
    "    \n",
    "    for sample in sample_order:\n",
    "        if sample in samples:\n",
    "            tmp_TSD.append('|'.join(TSDs[sample]))\n",
    "        else:\n",
    "            tmp_TSD.append('-')\n",
    "                        \n",
    "    for sample in sample_order:\n",
    "        if sample in samples:\n",
    "            tmp_remappable.append('|'.join(remappable_flag[sample]))\n",
    "            if 'True' in '|'.join(remappable_flag[sample]):\n",
    "                remappable_filter = 1\n",
    "        else:\n",
    "            tmp_remappable.append('-')\n",
    "                        \n",
    "    for sample in sample_order:\n",
    "        if sample in samples:\n",
    "            tmp_filter.append('|'.join(filter_flag[sample]))\n",
    "        else:\n",
    "            tmp_filter.append('-')\n",
    "            \n",
    "    for sample in sample_order:\n",
    "        if sample in samples:\n",
    "            tmp_telength.append('|'.join(telengths[sample]))\n",
    "        else:\n",
    "            tmp_telength.append('-')\n",
    "            \n",
    "    for sample in sample_order:\n",
    "        if sample in samples:\n",
    "            tmp_mapq.append('|'.join(mapqs[sample]))\n",
    "        else:\n",
    "            tmp_mapq.append('-')\n",
    "    \n",
    "    if coverage_filter == 0:\n",
    "        skip_coverage +=1\n",
    "        continue\n",
    "        \n",
    "    total_usedreads = sum([int(x) for x in tmp_usedreads])\n",
    "    total_coverage = sum([int(x) for x in tmp_coverage])\n",
    "    \n",
    "    for x in tmp_coverage:\n",
    "        if int(x) > coverage_max_threshold:\n",
    "            coverage_deep_filter = 1\n",
    "            break\n",
    "            \n",
    "    tmp_rrs = []\n",
    "    \n",
    "    for ind in range(len(sample_order)):\n",
    "        if int(tmp_coverage[ind]) < coverage_threshold:\n",
    "            continue\n",
    "        tmp_rrs.append(int(tmp_usedreads[ind]) / int(tmp_coverage[ind]) )\n",
    "        \n",
    "    rr = total_usedreads / total_coverage\n",
    "    rr_median = statistics.median(tmp_rrs)\n",
    "    \n",
    "    if (len(tmp_rrs) > 1) and ((max(rr, rr_median) / min(rr, rr_median))>= 3)  :\n",
    "        skip_rr += 1\n",
    "        continue\n",
    "        \n",
    "    rr = rr_median\n",
    "    \n",
    "    nsamples = len(samples)\n",
    "    \n",
    "    if coverage_deep_filter == 1:\n",
    "        skip_coverage_deep +=1\n",
    "        continue\n",
    "        \n",
    "    if total_usedreads == 1:\n",
    "        genotype = 'Singleton'\n",
    "        if remappable_filter == 0:\n",
    "            skip_remappable +=1\n",
    "            continue\n",
    "    elif (rr < 0.1) and (tissue == 'sboth'):\n",
    "        genotype = 'Rare'\n",
    "        if remappable_filter == 0:\n",
    "            skip_remappable +=1\n",
    "            continue\n",
    "    elif rr >=0.1 :\n",
    "        genotype = 'Fixed'\n",
    "\n",
    "    else:\n",
    "        genotype = 'Ungenotyped'\n",
    "                \n",
    "    instype = 'TBD'\n",
    "    \n",
    "    if ('FL' in preserved):\n",
    "        inspreserv = 'FL'\n",
    "    elif ('AFL' in preserved):\n",
    "        inspreserv = 'AFL'\n",
    "            \n",
    "    new_row = [';'.join(tmp_uuid), ';'.join(tmp_sample), \\\n",
    "               chrm, round(np.median(starts)), round(np.median(ends)), strand, \\\n",
    "              order, family, subfamily,\\\n",
    "               ';'.join(tmp_usedreads), ';'.join(tmp_spanreads), ';'.join(tmp_coverage), \\\n",
    "               ';'.join(tmp_TSD), ';'.join(tmp_remappable), ';'.join(tmp_filter), \\\n",
    "               ';'.join(tmp_telength), ';'.join(tmp_mapq), reflength, \n",
    "               nsamples, rr, genotype, instype, tissue, inspreserv, \\\n",
    "               np.min(starts), np.max(starts), 0.0, 0.0\n",
    "              ]        \n",
    "    tldr_merged_list.append(new_row)\n",
    "    index +=1\n",
    "    \n",
    "print('low coverage', skip_coverage)\n",
    "print('aberrant ontRR', skip_rr)\n",
    "print('remappable filter', skip_remappable)\n",
    "print('very high coverage', skip_coverage_deep)\n",
    "print('RR outlier ', count_rr_outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['UUID', 'Sample', \n",
    "           'Chrom', 'Start', 'End', 'Strand', \n",
    "           'Order', 'Family', 'Subfamily',\n",
    "          'UsedReads', 'SpanReads', 'Coverage', \n",
    "           'TSD', 'Remappable', 'Filter',  \n",
    "           'TELength', 'MapQ', 'RefLength',\n",
    "           'nSamples',\n",
    "           'ontRR', 'Genotype', 'Type', 'Tissue', 'Preserved', \n",
    "           'minRefStart', 'maxRefStart', 'illAF', 'illPF']\n",
    "tldr_merged = pd.DataFrame(tldr_merged_list, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_te = pd.read_csv('~/PGFP_refTE_dm6.csv', index_col=0, sep='\\t')\n",
    "\n",
    "ref_te['illAF'] = pd.Series(dtype='int')\n",
    "ref_te['illPF'] = pd.Series(dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_remove = []\n",
    "count_genotype = {}\n",
    "\n",
    "for indx, row in tldr_merged.iterrows():    \n",
    "    uuid = row['UUID']\n",
    "    chrm = row['Chrom']\n",
    "    start = row['Start']\n",
    "    end = row['End']\n",
    "    strand = row['Strand']\n",
    "    genotype = row['Genotype']\n",
    "    remap = row['Remappable']\n",
    "    fltr = row['Filter']\n",
    "    subfamily = row['Subfamily']\n",
    "    \n",
    "    if genotype not in count_genotype:\n",
    "        count_genotype[genotype] = {}\n",
    "\n",
    "    tmp = tldr_merged[(tldr_merged['Subfamily'] == subfamily) \n",
    "                 & (start >= (tldr_merged['Start'] - 1000)) & (start <= (tldr_merged['End'] + 1000))\n",
    "                 & (tldr_merged['Chrom'] == chrm) ]\n",
    "    \n",
    "    if len(tmp.index) > 1:\n",
    "        to_keep = []\n",
    "        to_remove = []\n",
    "        for indx2, row2 in tmp.iterrows():\n",
    "            pass_filter = row2['Filter']\n",
    "            tissue = row2['Tissue']\n",
    "            \n",
    "            if 'PASS' in pass_filter:\n",
    "                to_keep.append([indx2, tissue])\n",
    "            else:\n",
    "                to_remove.append([indx2, tissue])\n",
    "                            \n",
    "            if len(to_keep) > 1:\n",
    "                max_tissue = ''\n",
    "                good_indx = 0\n",
    "                for [j, tissue] in to_keep:\n",
    "                    if tissue == 'sboth':\n",
    "                        max_tissue = 'sboth'\n",
    "                        good_indx = j\n",
    "                \n",
    "                if max_tissue != 'sboth':\n",
    "                    pass # keeping all\n",
    "                else:\n",
    "                    for [j, tissue] in to_keep:\n",
    "                        if j!=good_indx:\n",
    "                            if j not in rows_to_remove:\n",
    "                                rows_to_remove.append(j)\n",
    "\n",
    "                                if subfamily not in count_genotype[genotype]:\n",
    "                                    count_genotype[genotype][subfamily] = 0\n",
    "                                count_genotype[genotype][subfamily] +=1\n",
    "            if len(to_keep) == 1:\n",
    "                for [j, tissue] in to_remove:\n",
    "                    if j not in rows_to_remove:\n",
    "                        rows_to_remove.append(j)\n",
    "                        \n",
    "                        if subfamily not in count_genotype[genotype]:\n",
    "                            count_genotype[genotype][subfamily] = 0\n",
    "                        count_genotype[genotype][subfamily] +=1\n",
    "            if len(to_keep) == 0:\n",
    "                good_indx = tmp['nSamples'].idxmax()\n",
    "                for [j, tissue] in to_remove:\n",
    "                    if j != good_indx:\n",
    "                        \n",
    "                        if j not in rows_to_remove:\n",
    "                            rows_to_remove.append(j)\n",
    "\n",
    "                            if subfamily not in count_genotype[genotype]:\n",
    "                                count_genotype[genotype][subfamily] = 0\n",
    "                            count_genotype[genotype][subfamily] +=1\n",
    "        \n",
    "tldr_merged.drop(rows_to_remove, inplace = True)\n",
    "tldr_merged.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tldr_merged.append(ref_te)\n",
    "result.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {'Rare' : [], 'Singleton' : [], 'Fixed': [], 'Ungenotyped' : []}\n",
    "for index, row in result.iterrows():\n",
    "    genotype = row['Genotype']\n",
    "    samples = row['Sample']\n",
    "\n",
    "    if genotype in ['Singleton', 'Rare', 'Fixed', 'Ungenotyped'] :\n",
    "        include = ((('gut' in samples) & ('head' not in samples)) | \\\n",
    "                   (('head' in samples) & ('gut' not in samples)) \\\n",
    "                  ) \n",
    "        if include :\n",
    "            chrm = row['Chrom']\n",
    "            minstart = row['minRefStart']\n",
    "            maxstart = row['maxRefStart']\n",
    "            strand = row['Strand']\n",
    "            name = '_'.join([row['Order'], row['Family'], row['Subfamily']]) \n",
    "            uuid = row['UUID']            \n",
    "            coords[genotype].append([chrm, minstart, maxstart, strand, name, samples, uuid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for genotype in ['Rare', 'Singleton', 'Unknown', 'Fixed']:\n",
    "    print(genotype)\n",
    "    outfiles = {}\n",
    "    \n",
    "    if not os.path.isdir(opath):\n",
    "        os.makedirs(opath)\n",
    "        \n",
    "    for coord in coords[genotype]:\n",
    "        print(coord)\n",
    "        chrm = coord[0]\n",
    "        minstart = coord[1]\n",
    "        maxstart = coord[2]\n",
    "        strand = coord[3]\n",
    "        name = coord[4]\n",
    "        ins_samples = coord[5]\n",
    "        flank = 200\n",
    "        records = []\n",
    "        outfile = '_'.join([chrm, str(minstart), str(maxstart), name, strand])\n",
    "        for sample in samples:\n",
    "            readcount = 200\n",
    "            if sample in ins_samples:\n",
    "                continue\n",
    "            bamfile = '~/' + sample + '.dm6.porechop.sorted.flt.bam'\n",
    "            bam = pysam.AlignmentFile(bamfile, 'rb')\n",
    "            read_set = bam.fetch(chrm, minstart - flank, maxstart + flank)\n",
    "        \n",
    "            for read in read_set:\n",
    "                if readcount < 0:\n",
    "                    break\n",
    "                readcount -= 1\n",
    "                if read.is_supplementary or read.is_unmapped or (read.mapping_quality < 5):\n",
    "                    continue\n",
    "                if not read.is_reverse:\n",
    "\n",
    "                    ref_start = read.get_reference_positions()[0]\n",
    "                    ref_end = read.get_reference_positions()[-1]\n",
    "\n",
    "                    if (ref_start >= minstart) and (ref_start <= maxstart + flank):\n",
    "                        for i, cigar in enumerate(read.cigartuples):\n",
    "                            if i == 0 and (cigar[0] in [4,5]): \n",
    "                                left_clipped_pos = cigar[1]\n",
    "                                seq = str(read.query_sequence[0:left_clipped_pos ] )\n",
    "                                if len(seq) < 100:\n",
    "                                    continue\n",
    "                                record = SeqRecord(\n",
    "                                    Seq(seq),\n",
    "                                    id=':'.join([sample, read.query_name, '+']),\n",
    "                                    name='',\n",
    "                                    description='')\n",
    "                                records.append(record)\n",
    "                                break\n",
    "\n",
    "                    if (ref_start < minstart) and (ref_start >= minstart - flank) :\n",
    "                        for i, cigar in enumerate(read.cigartuples):\n",
    "                            if i == 0 and (cigar[0] in [4,5]): \n",
    "                                left_clipped_pos = cigar[1]\n",
    "                                seq = str(read.query_sequence[0:left_clipped_pos] )\n",
    "                                if len(seq) < 100:\n",
    "                                    continue\n",
    "                                record = SeqRecord(\n",
    "                                    Seq(seq),\n",
    "                                    id=':'.join([sample, read.query_name, '+']),\n",
    "                                    name='',\n",
    "                                    description='')\n",
    "                                records.append(record)\n",
    "                                break\n",
    "                                \n",
    "                    if (ref_end < minstart) and (ref_end >= minstart - flank) :\n",
    "                        for i, cigar in reversed(list(enumerate(read.cigartuples))):\n",
    "                            if (i == (len(read.cigartuples) -1)) and (cigar[0] in [4,5]): \n",
    "                                right_clipped_pos = cigar[1]\n",
    "                                seq = str(read.query_sequence[(len(read.query_sequence) - right_clipped_pos):-1] )\n",
    "                                if len(seq) < 100:\n",
    "                                    continue\n",
    "                                record = SeqRecord(\n",
    "                                    Seq(seq),\n",
    "                                    id=':'.join([sample, read.query_name, '+']),\n",
    "                                    name='',\n",
    "                                    description='')\n",
    "                                records.append(record)\n",
    "                                break\n",
    "\n",
    "                    if (ref_end >= minstart) and (ref_end <= maxstart + flank):\n",
    "                        for i, cigar in reversed(list(enumerate(read.cigartuples))):\n",
    "                            if (i == (len(read.cigartuples) -1))  and (cigar[0] in [4,5]): \n",
    "                                right_clipped_pos = cigar[1]\n",
    "                                seq = str(read.query_sequence[(len(read.query_sequence) - right_clipped_pos ):-1] )\n",
    "                                if len(seq) < 100:\n",
    "                                    continue\n",
    "                                record = SeqRecord(\n",
    "                                    Seq(seq),\n",
    "                                    id=':'.join([sample, read.query_name, '+']),\n",
    "                                    name='',\n",
    "                                    description='')\n",
    "                                records.append(record)\n",
    "                                break\n",
    "\n",
    "                if read.is_reverse:\n",
    "                    ref_start = read.get_reference_positions()[0]\n",
    "                    ref_end = read.get_reference_positions()[-1]\n",
    "                    if (ref_start >= minstart) and (ref_start <= maxstart + flank):\n",
    "                        for i, cigar in enumerate(read.cigartuples):\n",
    "                            if (i == 0) and (cigar[0] in [4,5]): \n",
    "                                left_clipped_pos = cigar[1]\n",
    "                                seq = str(Seq(read.query_sequence[0:left_clipped_pos]).reverse_complement() )\n",
    "                                if len(seq) < 100:\n",
    "                                    continue\n",
    "                                record = SeqRecord(\n",
    "                                    Seq(seq),\n",
    "                                    id=':'.join([sample, read.query_name, '-']),\n",
    "                                    name='',\n",
    "                                    description='')\n",
    "                                records.append(record)\n",
    "                                break\n",
    "\n",
    "                    if (ref_start < minstart) and (ref_start >= minstart - flank) :\n",
    "                        for i, cigar in enumerate(read.cigartuples):\n",
    "                            if (i == 0) and (cigar[0] in [4,5]): \n",
    "                                left_clipped_pos = cigar[1]\n",
    "                                seq = str(Seq(read.query_sequence[0:left_clipped_pos]).reverse_complement() )\n",
    "                                if len(seq) < 100:\n",
    "                                    continue\n",
    "                                record = SeqRecord(\n",
    "                                    Seq(seq),\n",
    "                                    id=':'.join([sample, read.query_name, '-']),\n",
    "                                    name='',\n",
    "                                    description='')\n",
    "                                records.append(record)\n",
    "                                break\n",
    "\n",
    "                    if (ref_end < minstart) and (ref_end >= minstart - flank):\n",
    "                        for i, cigar in reversed(list(enumerate(read.cigartuples))):\n",
    "                            if (i == (len(read.cigartuples) -1)) and (cigar[0] in [4,5]): \n",
    "                                right_clipped_pos = cigar[1]\n",
    "                                seq = str(Seq(read.query_sequence[(len(read.query_sequence) - right_clipped_pos):-1]).reverse_complement() )\n",
    "                                if len(seq) < 100:\n",
    "                                    continue\n",
    "                                record = SeqRecord(\n",
    "                                    Seq(seq),\n",
    "                                    id=':'.join([sample, read.query_name, '-']),\n",
    "                                    name='',\n",
    "                                    description='')\n",
    "                                records.append(record)\n",
    "                                break\n",
    "\n",
    "                    if (ref_end >= minstart) and (ref_end <= maxstart + flank):\n",
    "                        for i, cigar in reversed(list(enumerate(read.cigartuples))):\n",
    "                            if (i == (len(read.cigartuples) -1) ) and (cigar[0] in [4,5]): \n",
    "                                right_clipped_pos = cigar[1]\n",
    "                                seq = str(Seq(read.query_sequence[(len(read.query_sequence) - right_clipped_pos):-1]).reverse_complement() )\n",
    "                                if len(seq) < 100:\n",
    "                                    continue\n",
    "                                record = SeqRecord(\n",
    "                                    Seq(seq),\n",
    "                                    id=':'.join([sample, read.query_name, '-']),\n",
    "                                    name='',\n",
    "                                    description='')\n",
    "                                records.append(record)\n",
    "                                break   \n",
    "\n",
    "        _ = SeqIO.write(records, outfile + '.fa', 'fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_genotypes = {}\n",
    "a = mp.Aligner(teTELR)  # load or build index\n",
    "families_updated = {}\n",
    "families_notupdated = {}\n",
    "notupdated_coords= {}\n",
    "count_strand_filter = 0\n",
    "genotype_map = {'Rare':'Rare', 'Singleton':'Singleton', 'Ungenotyped':'Unknown', 'Fixed':'Fixed'}\n",
    "\n",
    "for genotype in ['Rare', 'Singleton', 'Ungenotyped', 'Fixed']:    \n",
    "    print(genotype)\n",
    "    updated_bed = open('updated_coords_' + genotype + '.bed', 'w')\n",
    "    notupdated_bed = open('notupdated_coords_' + genotype + '.bed', 'w')\n",
    "    \n",
    "    notupdated_coords[genotype] = []\n",
    "    families_updated[genotype] = {}\n",
    "    families_notupdated[genotype] = {}\n",
    "    \n",
    "    single_sample = 0\n",
    "    total = 0\n",
    "    total_updated = 0\n",
    "    \n",
    "    to_check_upd = []\n",
    "    to_check_notupd = []\n",
    "    \n",
    "    for coord in coords[genotype]:\n",
    "        chrm = coord[0]\n",
    "        minstart = coord[1]\n",
    "        maxstart = coord[2]\n",
    "        te_strand = coord[3]\n",
    "        name = coord[4]\n",
    "        ins_samples = coord[5]\n",
    "        flank = 200\n",
    "        uuid = coord[-1]\n",
    "        samples_to_add = []\n",
    "        outfile = opath + '_'.join([chrm, str(minstart), str(maxstart), name, te_strand])\n",
    "        te_query = name.split('_')[2]\n",
    "        \n",
    "        total +=1\n",
    "\n",
    "        with open(apath + '_'.join([chrm, str(minstart), str(maxstart), name, genotype, te_strand]) + '.alg', 'w') as outdata:\n",
    "            \n",
    "            for read_name, seq, qual in mp.fastx_read(outfile  + '.fa'): # read a fasta/q sequence\n",
    "                    for hit in a.map(seq): # traverse alignments\n",
    "                        strand_filter = -1\n",
    "                        if hit.is_primary and hit.mapq >= 15:\n",
    "                            sample = read_name.split(':')[0]\n",
    "                            if sample in ins_samples:\n",
    "                                break\n",
    "                            te_read = hit.ctg.split('#')[0]\n",
    "                            read_strand = read_name.split(':')[2]\n",
    "                            hit_strand = hit.strand\n",
    "                            _ = outdata.write('\\t'.join([read_name, te_query, te_strand, read_strand,\n",
    "                                                 seq[hit.q_st:hit.q_en], hit.ctg, str(hit.r_st), str(hit.r_en), \n",
    "                                                 str(hit.mapq), hit.cigar_str, str(hit.strand)]) + '\\n')\n",
    "                        \n",
    "                            if te_strand == '+':\n",
    "                                if read_strand == '+':\n",
    "                                    if hit_strand == 1:\n",
    "                                        strand_filter = 1\n",
    "                                    elif hit_strand == -1:\n",
    "                                        strand_filter = 0\n",
    "                                elif read_strand == '-':\n",
    "                                    if hit_strand == 1:\n",
    "                                        strand_filter = 0\n",
    "                                    elif hit_strand == -1:\n",
    "                                        strand_filter = 1\n",
    "                            \n",
    "                            elif te_strand == '-':\n",
    "                                if read_strand == '+':\n",
    "                                    if hit_strand == 1:\n",
    "                                        strand_filter = 0\n",
    "                                    elif hit_strand == -1:\n",
    "                                        strand_filter = 1\n",
    "                                elif read_strand == '-':\n",
    "                                    if hit_strand == 1:\n",
    "                                        strand_filter = 1\n",
    "                                    elif hit_strand == -1:\n",
    "                                        strand_filter = 0\n",
    "                            \n",
    "                            if (te_read == te_query) and (sample not in samples_to_add) \\\n",
    "                                and (strand_filter == 1) and (not check_for_low_complexity(seq[hit.q_st:hit.q_en])):\n",
    "                                samples_to_add.append(sample)\n",
    "                                \n",
    "                            if strand_filter == 0:\n",
    "                                count_strand_filter+=1\n",
    "                            if strand_filter == -1:\n",
    "                                print('Error: strand_filter')\n",
    "                                break\n",
    "                                                                                             \n",
    "        if len(samples_to_add) == 0:\n",
    "            if genotype=='Fixed':\n",
    "                to_check_notupd.append(uuid)\n",
    "                \n",
    "            notupdated_coords[genotype].append(coord)\n",
    "            _ = notupdated_bed.write('\\t'.join([chrm, str(minstart), str(maxstart), '.', te_query, te_strand]) + '\\n')  \n",
    "\n",
    "            if te_query not in families_notupdated[genotype]:\n",
    "                families_notupdated[genotype][te_query] = 1\n",
    "            else:\n",
    "                families_notupdated[genotype][te_query] +=1\n",
    "\n",
    "            if len(ins_samples.split(';')) == 1:\n",
    "                single_sample += 1\n",
    "        else:\n",
    "            if genotype=='Fixed':\n",
    "                to_check_upd.append(uuid)\n",
    "            if uuid in updated_genotypes:\n",
    "                print('Error: uuid is already in updated_genotypes')\n",
    "                break\n",
    "            updated_genotypes[uuid]= 'add_samples:' + ';'.join(samples_to_add) \n",
    "            _ = updated_bed.write('\\t'.join([chrm, str(minstart), str(maxstart), '.', te_query, te_strand]) + '\\n')  \n",
    "            total_updated +=1\n",
    "            if te_query not in families_updated[genotype]:\n",
    "                families_updated[genotype][te_query] = 1\n",
    "            else:\n",
    "                families_updated[genotype][te_query] +=1\n",
    "    print(total, total_updated, total - total_updated, single_sample)\n",
    "\n",
    "    families_notupdated[genotype]\n",
    "    families_updated[genotype]\n",
    "    notupdated_bed.close()\n",
    "    updated_bed.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_keep = 0\n",
    "count_remove = 0\n",
    "count_somatic= 0 \n",
    "count_fail_arm = 0\n",
    "count_fail_ms = 0\n",
    "count_fail_other = 0\n",
    "               \n",
    "for coord in notupdated_coords['Ungenotyped']:\n",
    "    print(coord)\n",
    "    reads = []\n",
    "    families = []\n",
    "    uuids_unsplit = coord[-1]\n",
    "    uuids = coord[-1].split(';')\n",
    "    samples = coord[-2].split(';')\n",
    "    uuid_list = []\n",
    "    samples_list= []\n",
    "    pass_filter = []\n",
    "    \n",
    "    for i in range(len(samples)):\n",
    "        if samples[i]!= '-':\n",
    "            if '|' not in uuids[i]:\n",
    "                uuid_list.append(uuids[i])\n",
    "                samples_list.append(samples[i])\n",
    "            else:\n",
    "                for uuid in uuids[i].split('|'):\n",
    "                    uuid_list.append(uuid)\n",
    "                    samples_list.append(samples[i])\n",
    "    \n",
    "    for ind, uuid in enumerate(uuid_list):\n",
    "        read_count = 0\n",
    "        with open(uuid + '.detail.out') as indata:\n",
    "            for i, l in enumerate(indata):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                ln = l.rstrip().split('\\t')\n",
    "                if (ln[-3] == 'True') and (float(ln[-4]) >= 0.8):\n",
    "                    reads.append(ln[2])\n",
    "                    families.append(ln[4].split(',')[1])\n",
    "                read_count += 1\n",
    "            if uuids_unsplit in updated_genotypes:\n",
    "                print('WARNING: uuid is already in updated_genotypes')\n",
    "\n",
    "            print(read_count, len(reads), len(list(set(reads))), len(list(set(families))), end=' ')\n",
    "            \n",
    "            if (len(list(set(reads))) == 1) and (len(list(set(families))) == 1):\n",
    "                pass_filter.append('FAIL_ambigous_read_mapping')\n",
    "\n",
    "            elif (len(list(set(reads))) >= 1) and (len(list(set(families))) > 1):            \n",
    "                pass_filter.append('FAIL_multiple_subfamilies')\n",
    "\n",
    "            elif (len(list(set(reads))) >= 2) and (len(list(set(families))) == 1):            \n",
    "                pass_filter.append('PASS')\n",
    "            else:\n",
    "                pass_filter.append('FAIL_other')\n",
    "    \n",
    "    if 'PASS' in pass_filter:\n",
    "        updated_genotypes[uuids_unsplit] = 'keep'\n",
    "        count_keep +=1\n",
    "    else:    \n",
    "        count_remove+=1\n",
    "        updated_genotypes[uuids_unsplit] = 'remove'\n",
    "        \n",
    "        if 'FAIL_ambigous_read_mapping' in pass_filter:\n",
    "            count_fail_arm+=1\n",
    "        if 'FAIL_multiple_subfamilies' in pass_filter:\n",
    "            count_fail_ms+=1\n",
    "        if 'FAIL_other' in pass_filter:\n",
    "            count_fail_other+=1\n",
    "        \n",
    "print('count_keep', count_keep)\n",
    "print('count_remove', count_remove)\n",
    "print('FAIL_ambigous_read_mapping, probably singleton', count_fail_arm)\n",
    "print('FAIL_multiple_subfamilies', count_fail_ms)\n",
    "print('FAIL_other', count_fail_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_remove = []\n",
    "updt_rows = []\n",
    "count_changed_gt_fixed = 0\n",
    "count_changed_single_germvar = 0\n",
    "count_changed_single_noTSD = 0\n",
    "count_changed_single_longTSD = 0\n",
    "count_singleton_to_rare = 0\n",
    "count_singleton_to_ungenotyped = 0\n",
    "count_ungenotyped_to_rare = 0\n",
    "header = list(result.columns.values)\n",
    "\n",
    "for index, row in result.iterrows():\n",
    "    uuid = row['UUID']\n",
    "    genotype = row['Genotype']\n",
    "    samples = row['Sample']\n",
    "    tissue = row['Tissue']\n",
    "    subfamily = row['Subfamily']\n",
    "    nsamples = int(row['nSamples'])\n",
    "    vals = row.values.flatten().tolist()\n",
    "    flag_to_drop = 0\n",
    "    \n",
    "    if uuid in updated_genotypes:\n",
    "        action = updated_genotypes[uuid]\n",
    "        if (action=='remove') or (action=='remove_somatic'):\n",
    "            if index in rows_to_remove:\n",
    "                print('Warning: duplicated index in rows_to_remove')\n",
    "            rows_to_remove.append(index)\n",
    "            flag_to_drop = 1\n",
    "            \n",
    "        if action=='rare_to_somatic':\n",
    "            vals[header.index('Genotype')] = 'Somatic'\n",
    "            result.at[index, 'Genotype'] = 'Somatic'\n",
    "                        \n",
    "        if action == 'keep':\n",
    "            pass\n",
    "        if action == 'keep_somatic':\n",
    "            pass\n",
    "        \n",
    "        if 'add_samples:' in action:\n",
    "            samples_list = samples.split(';')\n",
    "            samples_toadd_list = action.split(':')[1].split(';')\n",
    "            \n",
    "            for i, sample in enumerate(sample_order):\n",
    "                if sample in samples_toadd_list:\n",
    "                    samples_list[i] = sample\n",
    "            \n",
    "            vals[header.index('Sample')] = ';'.join(samples_list)\n",
    "            vals[header.index('nSamples')] = nsamples + len(samples_toadd_list)\n",
    "            \n",
    "            result.at[index, 'Sample'] = ';'.join(samples_list)\n",
    "            result.at[index, 'nSamples'] = nsamples + len(samples_toadd_list)\n",
    "            \n",
    "            samples = ';'.join(samples_list)\n",
    "            \n",
    "            if ('gut' in samples.lower()) and ('head' in samples.lower()):\n",
    "                vals[header.index('Tissue')] = 'sboth'\n",
    "                result.at[index, 'Tissue'] = 'sboth'\n",
    "                tissue = 'sboth'\n",
    "            elif 'gut' in samples.lower():\n",
    "                vals[header.index('Tissue')] = 'gut'\n",
    "                result.at[index, 'Tissue'] = 'gut'\n",
    "                tissue = 'gut'\n",
    "            elif 'head' in samples.lower():\n",
    "                vals[header.index('Tissue')] = 'head'\n",
    "                result.at[index, 'Tissue'] = 'head'\n",
    "                tissue = 'head'\n",
    "            \n",
    "            if (genotype == 'Fixed') :\n",
    "                if (tissue == 'gut') or (tissue == 'head'):\n",
    "                    count_changed_gt_fixed+=1\n",
    "                    \n",
    "                    vals[header.index('Genotype')] = 'Ungenotyped'\n",
    "                    result.at[index, 'Genotype'] = 'Ungenotyped'\n",
    "                \n",
    "            if (genotype == 'Singleton'):\n",
    "                if tissue == 'sboth':\n",
    "                    vals[header.index('Genotype')] = 'Rare'\n",
    "                    result.at[index, 'Genotype'] = 'Rare'\n",
    "                    count_singleton_to_rare+=1\n",
    "                    \n",
    "                if (tissue == 'gut') or (tissue == 'head'):\n",
    "                    vals[header.index('Genotype')] = 'Ungenotyped'\n",
    "                    result.at[index, 'Genotype'] = 'Ungenotyped'\n",
    "                    count_singleton_to_ungenotyped+=1\n",
    "                    \n",
    "            if (genotype == 'Ungenotyped') :\n",
    "                if tissue == 'sboth':\n",
    "                    vals[header.index('Genotype')] = 'Rare'\n",
    "                    result.at[index, 'Genotype'] = 'Rare'\n",
    "                    count_ungenotyped_to_rare+=1\n",
    "                    \n",
    "                if (tissue == 'gut') or (tissue == 'head'):\n",
    "                    vals[header.index('Genotype')] = 'Ungenotyped'\n",
    "                    result.at[index, 'Genotype'] = 'Ungenotyped'\n",
    "    else:\n",
    "        \n",
    "        if (genotype == 'Singleton') and (subfamily in germline_active):\n",
    "            vals[header.index('Genotype')] = 'Ungenotyped'\n",
    "            result.at[index, 'Genotype'] = 'Ungenotyped'\n",
    "            count_changed_single_germvar+=1\n",
    "            \n",
    "        elif (genotype == 'Singleton') and (('noTSD' in row['TSD'])):\n",
    "            vals[header.index('Genotype')] = 'Ungenotyped'\n",
    "            result.at[index, 'Genotype'] = 'Ungenotyped'\n",
    "            count_changed_single_noTSD+=1\n",
    "        \n",
    "        elif (genotype == 'Singleton') and (('longTSD' in row['TSD'])  ):\n",
    "            vals[header.index('Genotype')] = 'Ungenotyped'\n",
    "            result.at[index, 'Genotype'] = 'Ungenotyped'\n",
    "            count_changed_single_longTSD+=1\n",
    "                        \n",
    "        elif (genotype == 'Fixed') :\n",
    "            if (tissue == 'gut') or (tissue == 'head'):\n",
    "                count_changed_gt_fixed+=1\n",
    "                \n",
    "                vals[header.index('Genotype')] = 'Ungenotyped'\n",
    "                result.at[index, 'Genotype'] = 'Ungenotyped'\n",
    "                \n",
    "    if not flag_to_drop:\n",
    "        updt_rows.append(vals)\n",
    "\n",
    "print('count_changed_gt_fixed=', count_changed_gt_fixed)\n",
    "print('count_changed_single_germvar=', count_changed_single_germvar)\n",
    "print('count_changed_single_noTSD=', count_changed_single_noTSD)\n",
    "print('count_changed_single_longTSD=', count_changed_single_longTSD)\n",
    "print('count_singleton_to_rare', count_singleton_to_rare)\n",
    "print('count_singleton_to_ungenotyped', count_singleton_to_ungenotyped)\n",
    "print('count_ungenotyped_to_rare', count_ungenotyped_to_rare)\n",
    "print('number of rows to remove=', len(rows_to_remove))\n",
    "\n",
    "result.drop(index=rows_to_remove, inplace = True)       \n",
    "result.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_singleton_noTSD = 0\n",
    "count_singleton_longTSD = 0\n",
    "count_singleton_gv = 0\n",
    "count_pot_clonal = 0\n",
    "count_amb_samples = 0\n",
    "count_other = 0\n",
    "\n",
    "for i, row in result[(result['Genotype']=='Ungenotyped')].iterrows():\n",
    "    genotype = row['Genotype']\n",
    "    tissue = row['Tissue']\n",
    "    subfamily = row['Subfamily']\n",
    "    tissue = row['Tissue']\n",
    "    nSamples = int(row['nSamples'])\n",
    "    tsd = row['TSD']\n",
    "    usedReads = row['UsedReads'].replace('0', '').replace(';', '')\n",
    "    \n",
    "    if (nSamples==1) and  (int(usedReads)==1) and (subfamily in germline_active):\n",
    "        count_singleton_gv+=1\n",
    "    \n",
    "    elif (nSamples==1) and  (int(usedReads)==1) and ('noTSD' in tsd):\n",
    "        count_singleton_noTSD+=1\n",
    "        \n",
    "    elif (nSamples==1) and  (int(usedReads)==1) and ('longTSD' in tsd):\n",
    "        count_singleton_longTSD+=1\n",
    "        \n",
    "    elif (nSamples==1) and (int(usedReads)>1):\n",
    "        count_pot_clonal +=1\n",
    "    \n",
    "    elif (nSamples>1) and (tissue!='sboth'):\n",
    "        count_amb_samples+=1\n",
    "    else:\n",
    "        count_other+=1\n",
    "        \n",
    "print('count_singleton_noTSD', count_singleton_noTSD)\n",
    "print('count_singleton_longTSD', count_singleton_longTSD)\n",
    "print('count_singleton_gv', count_singleton_gv)\n",
    "print('count_pot_clonal', count_pot_clonal)\n",
    "print('count_amb_samples', count_amb_samples)\n",
    "print('count_other', count_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illumina = pd.read_csv('PGFP_Illumina.csv', index_col=0, header=0, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['illGT'] = pd.Series(dtype='int')\n",
    "result['illGT'] = result['illGT'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indx, row in result.iterrows():\n",
    "    uuid = row['UUID']\n",
    "    genotype = row['Genotype']\n",
    "    samples = row['Sample']\n",
    "    tissue = row['Tissue']\n",
    "    nsamples = int(row['nSamples'])\n",
    "    \n",
    "    chrm = row['Chrom']\n",
    "    start = row['Start']\n",
    "    strand = row['Strand']\n",
    "    subfamily = row['Subfamily']\n",
    "    tmp = illumina[(illumina['Chrom'] == chrm) & \n",
    "                   (start >= illumina['Start'] - 100 ) & \n",
    "                   (start <= illumina['Start']  + 100 ) &\n",
    "                   (illumina['Subfamily'] == subfamily ) &\n",
    "                  ((illumina['Strand'] == strand ) | (illumina['Strand'] == np.nan))\n",
    "                  ]\n",
    "    \n",
    "    if len(tmp.index) >1:\n",
    "        tmp\n",
    "    \n",
    "    if len(tmp.index) == 1:\n",
    "        result.at[indx, 'illAF'] = tmp['meanAF'].iloc[0]\n",
    "        result.at[indx, 'illPF'] = tmp['nPairs'].iloc[0] / 31\n",
    "        result.at[indx, 'illGT'] = tmp['Genotype'].iloc[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('~/final_table.csv', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
